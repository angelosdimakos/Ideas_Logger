# `dashboard`


## `dashboard\__init__`

**üß† Docstring Summary**

| Section | Content |
|---------|---------|
| Description | *No module description available.* |
| Args | ‚Äî |
| Returns | ‚Äî |


## `dashboard\ai_integration`

**üß† Docstring Summary**

| Section | Content |
|---------|---------|
| Description | *No module description available.* |
| Args | ‚Äî |
| Returns | ‚Äî |

### üì¶ Classes
#### `AIIntegration`
*No description available.*

### üõ†Ô∏è Functions
#### `__init__`
Initializes the AIIntegration instance with configuration and summarizer components.

#### `generate_audit_summary`
Generates an AI-driven audit summary based on provided metrics context.
Combines a persona-enriched audit summary prompt with the given metrics context and returns a summarized audit report as a string.

#### `generate_refactor_advice`
Generates AI-driven refactoring advice based on analysis of merged code data.
Analyzes the provided merged data to identify the top offenders for refactoring, constructs a contextual prompt, and returns a summary suggestion along with the list of top offenders.
**Parameters:**
merged_data: Aggregated code analysis data to be evaluated.
limit: The maximum number of top offenders to consider.
**Returns:**
A tuple containing the AI-generated refactor suggestion and the list of top offenders.

#### `generate_strategic_recommendations`
Generates strategic recommendations based on merged code analysis data.
Writes the merged data to a temporary JSON file and invokes a CLI assistant in strategic mode with the specified limit and persona. Returns the output generated by the CLI assistant.
**Parameters:**
merged_data: The combined code analysis data to be evaluated.
limit: The maximum number of recommendations to generate.
**Returns:**
The output string containing strategic recommendations.

#### `chat_general`
Generates an AI-driven summary response to a user query based on analyzed code report data.
**Parameters:**
user_query: The user's question or prompt for the AI.
merged_data: Aggregated code analysis data to inform the response.
**Returns:**
A summary string providing advice or insights relevant to the user query and code analysis context.

#### `chat_code`
Generates an AI-driven code analysis summary for a specific file based on user input.
Builds a detailed context using the file's complexity and linting information, issue locations, placeholder module summaries, and AI-generated refactor recommendations. Incorporates the user's query and persona to produce a comprehensive code analysis summary for the file.
**Parameters:**
file_path: Path to the file being analyzed.
complexity_info: Complexity metrics or data for the file.
lint_info: Linting quality information for the file.
user_query: User's question or prompt related to the file.
**Returns:**
A summarized AI-generated analysis of the file in response to the user query.

#### `chat_doc`
Generates a summary of a module's documentation using the provided functions list.
**Parameters:**
module_path: Path to the module to be summarized.
funcs: List of functions within the module to include in the summary.
**Returns:**
A summary string describing the module and its functions.


## `dashboard\app`

**üß† Docstring Summary**

| Section | Content |
|---------|---------|
| Description | *No module description available.* |
| Args | ‚Äî |
| Returns | ‚Äî |

### üõ†Ô∏è Functions
#### `init_artifacts_dir`
Returns the directory to use for artifacts, preferring the given default if it exists.
If the specified default directory does not exist, returns the current directory instead.
**Parameters:**
default_dir: The preferred directory for artifacts.
**Returns:**
The path to the artifacts directory.


## `dashboard\data_loader`

**üß† Docstring Summary**

| Section | Content |
|---------|---------|
| Description | *No module description available.* |
| Args | ‚Äî |
| Returns | ‚Äî |

### üõ†Ô∏è Functions
#### `is_excluded`
Determines whether a file path should be excluded based on predefined patterns.
**Returns:**
True if the path matches any exclusion pattern or is an '__init__.py' file; otherwise, False.

#### `load_artifact`
Loads a JSON artifact from the specified path, supporting compressed and specialized formats.
Attempts to load a coverage-related JSON artifact from the given path, handling `.comp.json.gz`, `.comp.json`, and plain `.json` variants. Applies specialized decompression for known report formats and filters out top-level keys matching exclusion criteria.
**Parameters:**
path: The base file path (without extension) of the artifact to load.
**Returns:**
A dictionary containing the processed and filtered artifact data, or an empty dictionary if no valid file is found.

#### `weighted_coverage`
Calculates the lines-of-code weighted average coverage from function coverage data.
**Parameters:**
func_dict: A dictionary where each value is a mapping containing "lines" (number of lines) and "coverage" (coverage ratio) for a function.
**Returns:**
The weighted average coverage ratio, or 0.0 if the total line count is zero.


## `dashboard\metrics`

**üß† Docstring Summary**

| Section | Content |
|---------|---------|
| Description | Module: scripts/dashboard/metrics.py
Extracts all data-transformation and metrics logic from the Streamlit app. |
| Args | ‚Äî |
| Returns | ‚Äî |

### üõ†Ô∏è Functions
#### `compute_executive_summary`
Generates high-level summary metrics for the dashboard's Executive Summary.
Aggregates unique test counts, average strictness and severity scores, number of production files, overall coverage percentage, and percentage of missing documentation from the provided data sources.
**Parameters:**
merged_data: Dictionary containing merged module data, including docstring information.
strictness_data: Dictionary containing strictness and coverage metrics for modules.
**Returns:**
A dictionary with total unique tests, average strictness, average severity, production file count, overall coverage percentage, and percentage of missing documentation.

#### `get_low_coverage_modules`
Returns the modules with the lowest coverage percentages.
Iterates over modules in the strictness data, excluding filtered files, and collects their coverage values. Returns a list of (module name, coverage) tuples for the modules with the lowest coverage, sorted in ascending order.
**Parameters:**
strictness_data: Dictionary containing module coverage information.
top_n: Number of modules to return.
**Returns:**
A list of tuples, each containing a module name and its coverage percentage.

#### `coverage_by_module`
Calculates line-of-code weighted coverage for each module and returns the modules with the lowest coverage.
**Parameters:**
merged_data: Dictionary containing module data, including coverage and complexity information.
top_n: Number of modules with the lowest coverage to return.
**Returns:**
A list of tuples containing the module name and its coverage percentage, sorted in ascending order by coverage.

#### `compute_severity`
Calculates a severity score for a file based on linting errors, code complexity, and coverage.
The severity score combines the number of mypy errors, pydocstyle lint issues, average function complexity, and coverage ratio using weighted factors. Returns a dictionary summarizing the file's name, path, error counts, average complexity, average coverage percentage, and computed severity score.

#### `compute_severity_df`
Builds a DataFrame summarizing severity metrics for all files.
Applies the provided severity computation function to each file in the merged data and constructs a DataFrame from the results, sorted by severity score in descending order with the index reset.
**Parameters:**
merged_data: A dictionary mapping file paths to their associated data.
compute_severity_fn: A function that computes severity metrics for a given file.
**Returns:**
A pandas DataFrame containing severity metrics for each file, sorted by severity score descending.

#### `build_prod_to_tests_df`
Creates a DataFrame mapping each production module to its unique covering tests and related metrics.
Deduplicates tests by name within each module, retaining the highest severity and corresponding strictness for each test. Calculates the average strictness and severity across unique tests per module, and lists the names of all covering tests. The resulting DataFrame includes the production module name, test count, average strictness, average severity, and a comma-separated list of test names, sorted by test count in descending order.

#### `severity_distribution`
Categorizes tests into Low, Medium, and High severity buckets based on their highest observed severity.
Deduplicates tests globally by test name, retaining only the highest severity for each test, and returns a count of tests in each severity category.
