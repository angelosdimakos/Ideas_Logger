import ollama
import traceback
import logging
from config_loader import load_config, get_config_value

class AISummarizer:
    def __init__(self):
        """
        Initializes the AISummarizer class by loading configuration values.
        Sets up the LLM model and prompt configurations.
        """
        config = load_config()
        self.model = get_config_value(config, "llm_model", "llama3")
        self.prompts_by_subcategory = get_config_value(config, "prompts_by_subcategory", {})
        print(f"[INFO] Initializing AISummarizer with model: {self.model}")
    
    
    def summarize_entry(self, entry_text, subcategory=None):
        """
        Generate a summary for a single entry based on its associated subcategory.
        
        Args:
            entry_text (str): The text of the entry to be summarized.
            subcategory (str, optional): The subcategory that influences the summarization prompt.
        
        Returns:
            str: The summarized response generated by the AI model.
        """
        prompt = self.prompts_by_subcategory.get(subcategory, self.prompts_by_subcategory["_default"])
        full_prompt = f"{prompt}\n\n{entry_text}"
        try:
            print(f"[AI] Single-entry prompt:\n{full_prompt}\n")
            response = ollama.generate(model=self.model, prompt=full_prompt)
            answer = response['response'].strip()
            print(f"[AI] Single-entry response:\n{answer}\n")
            return answer
        except Exception as e:
            error_msg = f"[❌ AI Error] summarize_entry failed: {str(e)}"
            print(error_msg)
            print(traceback.format_exc())
            logging.error(error_msg)
            
            # Try fallback approach
            try:
                print("[AI] Attempting fallback approach (chat) for single entry")
                response = ollama.chat(model=self.model, messages=[
                    {'role': 'user', 'content': full_prompt}
                ])
                if response and 'message' in response and 'content' in response['message']:
                    fallback_answer = response['message']['content'].strip()
                    print(f"[AI-Fallback] Single-entry:\n{fallback_answer}\n")
                    return fallback_answer
                else:
                    print("[❌ AI Error] Fallback approach returned unexpected format")
                    return None
            except Exception as fallback_error:
                print(f"[❌ AI Error] Fallback approach also failed: {str(fallback_error)}")
                return None

    def summarize_entries_bulk(self, entries, subcategory=None):
        """
        Summarizes multiple log entries as a batch for improved context awareness.
        
        Args:
            entries (list of str): A list of entry texts to be summarized.
            subcategory (str, optional): The subcategory that influences the summarization prompt.
        
        Returns:
            str: The summarized response for the bulk of entries generated by the AI model.
        """
        if not entries:
            print("[⚠️ Empty Input] summarize_entries_bulk received empty entry list")
            return None

        prompt_intro = self.prompts_by_subcategory.get(subcategory, self.prompts_by_subcategory["_default"])
        combined_text = "\n".join(f"- {entry}" for entry in entries)
        full_prompt = f"{prompt_intro}\n\n{combined_text}"

        print(f"[AI] Bulk prompt:\n{full_prompt}\n")
        try:
            response = ollama.generate(model=self.model, prompt=full_prompt)
            result = response['response'].strip()
            print(f"[AI] Bulk response:\n{result}\n")
            return result
        except Exception as e:
            error_msg = f"[❌ AI Error] summarize_entries_bulk failed: {str(e)}"
            print(error_msg)
            print(traceback.format_exc())
            logging.error(error_msg)
            
            # Attempt fallback
            try:
                print("[AI] Attempting fallback approach (chat) for bulk entries")
                response = ollama.chat(model=self.model, messages=[
                    {'role': 'user', 'content': full_prompt}
                ])
                if response and 'message' in response and 'content' in response['message']:
                    fallback_answer = response['message']['content'].strip()
                    print(f"[AI-Fallback] Bulk:\n{fallback_answer}\n")
                    return fallback_answer
                else:
                    return None
            except Exception as fallback_error:
                print(f"[❌ AI Error] Fallback approach also failed: {str(fallback_error)}")
                return None
