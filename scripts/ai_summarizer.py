import ollama
import traceback
import logging
from scripts.config_loader import load_config, get_config_value

class AISummarizer:
    def __init__(self):
        """
        Initializes the AISummarizer class by loading configuration values.
        Sets up the LLM model and prompt configurations.
        """
        config = load_config()
        self.model = get_config_value(config, "llm_model", "llama3")
        self.prompts_by_subcategory = get_config_value(config, "prompts_by_subcategory", {})
        print(f"[INFO] Initializing AISummarizer with model: {self.model}")
    
    def _fallback_summary(self, full_prompt):
        """
        Fallback method to summarize using `ollama.chat` in case of failure.
        """
        print("[AI] Attempting fallback approach (chat)")
        try:
            response = ollama.chat(model=self.model, messages=[
                {'role': 'user', 'content': full_prompt}
            ])
            if response and 'message' in response and 'content' in response['message']:
                fallback_answer = response['message']['content'].strip()
                print(f"[AI-Fallback] Fallback summary:\n{fallback_answer}\n")
                return fallback_answer
            else:
                print("[❌ AI Error] Fallback approach returned unexpected format")
                return "Fallback failed: Unexpected format"
        except Exception as fallback_error:
            print(f"[❌ AI Error] Fallback approach also failed: {str(fallback_error)}")
            return "Fallback failed: Ollama down"

    def summarize_entry(self, entry_text, subcategory=None):
        """
        Generate a summary for a single entry based on its associated subcategory.
        
        Args:
            entry_text (str): The text of the entry to be summarized.
            subcategory (str, optional): The subcategory that influences the summarization prompt.
        
        Returns:
            str: The summarized response generated by the AI model.
        """
        prompt = self.prompts_by_subcategory.get(subcategory, self.prompts_by_subcategory["_default"])
        full_prompt = f"{prompt}\n\n{entry_text}"
        try:
            print(f"[AI] Single-entry prompt:\n{full_prompt}\n")
            response = ollama.generate(model=self.model, prompt=full_prompt)
            answer = response['response'].strip()
            print(f"[AI] Single-entry response:\n{answer}\n")
            return answer
        except Exception as e:
            error_msg = f"[❌ AI Error] summarize_entry failed: {str(e)}"
            print(error_msg)
            print(traceback.format_exc())
            logging.error(error_msg)
            return self._fallback_summary(full_prompt)  # Use fallback on failure

    def summarize_entries_bulk(self, entries, subcategory=None):
        """
        Summarizes multiple log entries as a batch for improved context awareness.
        
        Args:
            entries (list of str): A list of entry texts to be summarized.
            subcategory (str, optional): The subcategory that influences the summarization prompt.
        
        Returns:
            str: The summarized response for the bulk of entries generated by the AI model.
        """
        if not entries:
            print("[⚠️ Empty Input] summarize_entries_bulk received empty entry list")
            return "No entries provided"

        prompt_intro = self.prompts_by_subcategory.get(subcategory, self.prompts_by_subcategory["_default"])
        combined_text = "\n".join(f"- {entry}" for entry in entries)
        full_prompt = f"{prompt_intro}\n\n{combined_text}"

        print(f"[AI] Bulk prompt:\n{full_prompt}\n")
        
        try:
            # Try generating summary using the AI model
            response = ollama.generate(model=self.model, prompt=full_prompt)
            # Check if the response structure is valid
            if 'response' in response:
                result = response['response'].strip()
                print(f"[AI] Bulk response:\n{result}\n")
                return result
            else:
                # Explicitly raise an exception for invalid response format
                raise ValueError("Invalid response format from Ollama API")
        
        except Exception as e:
            # Log the error and fallback if there was an issue
            error_msg = f"[❌ AI Error] summarize_entries_bulk failed: {str(e)}"
            print(error_msg)
            print(traceback.format_exc())
            logging.error(error_msg)

            # Explicitly ensure fallback is called when an error occurs
            fallback_result = self._fallback_summary(full_prompt)
            if not fallback_result:
                raise RuntimeError("Fallback failed to generate a summary.")
            return fallback_result  # Ensure fallback is triggered properly


